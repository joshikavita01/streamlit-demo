{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_Ques3(a)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshikavita01/streamlit-demo/blob/master/Assignment3_Ques3(a).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu==1.14"
      ],
      "metadata": {
        "id": "_WDT67Zk9BOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5MJzQeYpgmU",
        "outputId": "d3e3862c-ab41-470a-9705-b1d432fdfd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/gdrive/MyDrive/cityscapes_data.zip\" -d \"/content/\" "
      ],
      "metadata": {
        "id": "9HGSDuaXppiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Conv2DTranspose, Cropping2D\n",
        "from keras.layers import Input, Add, Dropout, Permute, add"
      ],
      "metadata": {
        "id": "DgX9c_2uucpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import  tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "KVXq7TSKJOYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists('/content/train') == False:\n",
        "  os.mkdir('/content/train')\n",
        "if os.path.exists('/content/val/') == False:\n",
        "  os.mkdir('/content/val/')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZBl9dQo1ucEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for folder in os.listdir(-'/content/cityscapes_data'):\n",
        "  _path = os.path.join('/content/cityscapes_data', folder)\n",
        "  for i in os.listdir(_path):\n",
        "    img_src = os.path.join(_path, i)\n",
        "    imgge = cv2.imread(img_src)\n",
        "    h, w, c = imgge.shape\n",
        "    x = imgge[0:h, 0:w//2]\n",
        "    y = imgge[0:h, w//2:w]\n",
        "    cv2.imwrite('/content/{}/{}'.format(folder, i),x)\n"
      ],
      "metadata": {
        "id": "SvhRAr977izQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path\n",
        "import scipy.misc\n",
        "import shutil\n",
        "import zipfile\n",
        "import time\n",
        "from glob import glob\n",
        "from urllib.request import urlretrieve\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class DL_Progress(tqdm):\n",
        "\tlast_block = 0\n",
        "\n",
        "\tdef ho_ok(self, block_num=1, block_size=1, total_size=None):\n",
        "\t\t\n",
        "\t\tself.total = total_size\n",
        "\t\tself.update((block_num - self.last_block) * block_size) \n",
        "\t\tself.last_block = block_num\n",
        "\n",
        "\n",
        "def pretrained_vgg(data_dir):\n",
        "\t\n",
        "\t_filename = 'vgg.zip'\n",
        "\t_path = os.path.join(data_dir, 'vgg')\n",
        "\tvgg_files = [\n",
        "\t\tos.path.join(_path, 'variables/variables.data-00000-of-00001'),\n",
        "\t\tos.path.join(_path, 'variables/variables.index'),\n",
        "\t\tos.path.join(_path, 'saved_model.pb')]\n",
        "\n",
        "\tmissing_vgg_files = [vgg_file for vgg_file in vgg_files if not os.path.exists(vgg_file)]\n",
        "\tif missing_vgg_files:\n",
        "\t\t\n",
        "\t\tif os.path.exists(_path):\n",
        "\t\t\tshutil.rmtree(_path)\n",
        "\t\tos.makedirs(_path)\n",
        "\n",
        "\t\n",
        "\t\twith DL_Progress(unit='B', unit_scale=True, miniters=1) as pbar:\n",
        "\t\t\turlretrieve(\n",
        "\t\t\t\t'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/vgg.zip',\n",
        "\t\t\t\tos.path.join(_path, _filename),\n",
        "\t\t\t\tpbar.hook)\n",
        "\t\tzip_ref = zipfile.ZipFile(os.path.join(_path, _filename), 'r')\n",
        "\t\tzip_ref.extractall(data_dir)\n",
        "\t\tzip_ref.close()\n",
        "\t\tos.remove(os.path.join(_path, _filename))\n",
        "\n",
        "\n",
        "def batch_(data_folder, image_shape):\n",
        "\t\n",
        "\tdef batches_fn(batch_size):\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t_paths = glob(os.path.join(data_folder, 'image_2', '*.png'))\n",
        "\t\tlabel_paths = {\n",
        "\t\t\tre.sub(r'_(lane|road)_', '_', os.path.basename(path)): path\n",
        "\t\t\tfor path in glob(os.path.join(data_folder, 'gt_image_2', '*_road_*.png'))}\n",
        "\t\tbg_color = np.array([255, 0, 0])\n",
        "\n",
        "\t\t\n",
        "\t\trandom.shuffle(_paths)\n",
        "\t\t\n",
        "\t\tfor batch_i in range(0, len(_paths), batch_size):\n",
        "\t\t\timages = []\n",
        "\t\t\t_images = []\n",
        "\t\t\tfor image_file in _paths[batch_i:batch_i+batch_size]:\n",
        "\t\t\t\t_image_file = label_paths[os.path.basename(image_file)]\n",
        "\t\t\t\timage = scipy.misc.imresize(scipy.misc.imread(image_file), image_shape)\n",
        "\t\t\t\tgt_image = scipy.misc.imresize(scipy.misc.imread(_image_file), image_shape)\n",
        "\t\t\t\tgt_bgg = np.all(gt_image == bg_color, axis=2)\n",
        "\t\t\t\tgt_bgg = gt_bgg.reshape(*gt_bgg.shape, 1)\n",
        "\t\t\t\tgt_image = np.concatenate((gt_bgg, np.invert(gt_bgg)), axis=2)\n",
        "\n",
        "\t\t\t\timages.append(image)\n",
        "\t\t\t\t_images.append(gt_image)\n",
        "\n",
        "\t\t\tyield np.array(images), np.array(_images)\n",
        "\treturn batches_fn\n",
        "\n",
        "\n",
        "def testt_output(sess, logits, keep_prob, image_pl, data_folder, image_shape):\n",
        "\t\n",
        "\tfor _file in glob(os.path.join(data_folder, 'image_2', '*.png')):\n",
        "\t\ti = scipy.misc.imresize(scipy.misc.imread(_file), image_shape)\n",
        "\n",
        "\t\t\n",
        "\t\tim_softmax = sess.run(\n",
        "\t\t\t[tf.nn.softmax(logits)],\n",
        "\t\t\t{keep_prob: 1.0, image_pl: [i]})\n",
        "\t\t\n",
        "\t\t_softmax = im_softmax[0][:, 1].reshape(image_shape[0], image_shape[1])\n",
        "\t\n",
        "\t\tseg = (im_softmax > 0.5).reshape(image_shape[0], image_shape[1], 1)\n",
        "\t\t\n",
        "\t\tm = np.dot(seg, np.array([[0, 255, 0, 127]]))\n",
        "\t\tm = scipy.misc.toimage(m, mode=\"RGBA\")\n",
        "\t\tstreet_im = scipy.misc.toimage(i)\n",
        "\t\tstreet_im.paste(m, box=None, mask=m)\n",
        "\n",
        "\t\tyield os.path.basename(_file), np.array(street_im)\n",
        "\n",
        "\n",
        "def _samples(_dir, data_, s, _shape, logits, _prob, input_):\n",
        "\toutput_dir = os.path.join(_dir, str(time.time()))\n",
        "\tif os.path.exists(_dir):\n",
        "\t\tshutil.rmtree(_dir)\n",
        "\tos.makedirs(_dir)\n",
        "\n",
        "\tprint(' Save test images: {}'.format(_dir))\n",
        "  _outputs = testt_output(\n",
        "\t\tsess, logits, keep_prob, _image, os.path.join(data_, 'data_road/testing'), _shape)\n",
        "\tfor name, image in _outputs:\n",
        "\t\tscipy.misc.imsave(os.path.join(_dir, name), image)"
      ],
      "metadata": {
        "id": "h6ZEqN0oNYT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "BATCH_SIZE = 25\n",
        "NUM_CLASSES=29\n",
        "IS_TRAIN = True\n",
        "KEEP_PROB = 0.5\n",
        "LEARNING_RATE = 0.0005\n",
        "L2_REG = 1e-2\n",
        "STDEV = 1e-3\n",
        "\n",
        "DATA_DIR = '/content/train'\n",
        "VIDEO_DIR = 'data\\leftImg8bit_demoVideo\\leftImg8bit\\demoVideo\\stuttgart_02'\n",
        "RUNS_DIR = 'runs'\n",
        "\n",
        "DEFAULT_SAVE_DIR = ''\n",
        "PRETRAINED_MODEL_DIR = ''"
      ],
      "metadata": {
        "id": "mcY9PapRNfy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from collections import namedtuple\n",
        "import random\n",
        "import numpy as np\n",
        "import os.path\n",
        "import scipy.misc\n",
        "import os\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import time\n",
        "\n",
        "def label_info():\n",
        "    Label = namedtuple('Label', [\n",
        "\n",
        "        'name',  \n",
        "        'id',  \n",
        "\n",
        "        'trainId',  \n",
        "\n",
        "        'category',  \n",
        "\n",
        "        'categoryId', \n",
        "\n",
        "        'hasInstances', \n",
        "\n",
        "        'ignoreInEval',  \n",
        "\n",
        "        'color', \n",
        "    ])\n",
        "\n",
        "    l= [\n",
        "        Label('unlabeled', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('ego vehicle', 1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('out of roi', 3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('static', 4, 255, 'void', 0, False, True, (0, 0, 0)),\n",
        "        Label('dynamic', 5, 255, 'void', 0, False, True, (111, 74, 0)),\n",
        "        Label('ground', 6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
        "        Label('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
        "        Label('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
        "        Label('parking', 9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
        "        Label('rail track', 10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
        "        Label('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
        "        Label('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
        "        Label('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
        "        Label('guard rail', 14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
        "        Label('bridge', 15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
        "        Label('tunnel', 16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
        "        Label('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
        "        Label('polegroup', 18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
        "        Label('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
        "        Label('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
        "        Label('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
        "        Label('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
        "        Label('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
        "        Label('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
        "        Label('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
        "        Label('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
        "        Label('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
        "        Label('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
        "        Label('caravan', 29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
        "        Label('trailer', 30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
        "        Label('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
        "        Label('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
        "        Label('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
        "        Label('license plate', -1, -1, 'vehicle', 7, False, True, (0, 0, 142)),\n",
        "    ]\n",
        "\n",
        "    s = set()\n",
        "    _list = list(map(lambda x : x[7], l))\n",
        "    _values = [x for x in _list if not (x in s or s.add(x))]\n",
        "\n",
        "    return _values\n",
        "\n",
        "def get_data(dp):\n",
        "    tp = dp + '/leftImg8bit/train/'\n",
        "    typ = dp+ '/sky-data/train/'\n",
        "\n",
        "    vp = dp + '/leftImg8bit/val/'\n",
        "    vyp = dp + '/sky-data/val/'\n",
        "\n",
        "    tb = glob(os.path.join(tp, '*/*.png'))\n",
        "\n",
        "    tb = glob(os.path.join(typ, '*/*color.png'))\n",
        "    vp = glob(os.path.join(vp, '*/*.png'))\n",
        "\n",
        "    vyp = glob(os.path.join(vyp, '*/*color.png'))\n",
        "\n",
        "    X_tr = []\n",
        "    y_tr = []\n",
        "    X_v = []\n",
        "    y_v = []\n",
        "\n",
        "    \n",
        "    for s in tb:\n",
        "        img_path = s\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        X_tr.append(x)\n",
        "\n",
        "    \n",
        "    for s in typ:\n",
        "        img_path = s\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        y_tr.append(x)\n",
        "\n",
        "    \n",
        "    for s in vp:\n",
        "        img_path = s\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        X_v.append(x)\n",
        "\n",
        "    \n",
        "    for s in vyp:\n",
        "        img_path = s\n",
        "        x = load_image(img_path)\n",
        "        x = cv2.resize(x, dsize=(512, 256))\n",
        "        y_v.append(x)\n",
        "\n",
        "    return X_tr, y_tr, X_v, y_v\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Aslvbi7YN7KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import helper\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n",
        "def load_vgg(sess, vgg_path):\n",
        "   \n",
        "    vgg_tag = 'vgg16'\n",
        "    vgg_input_tensor_name = 'image_input:0'\n",
        "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
        "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
        "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
        "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
        "\n",
        "    graph = tf.get_default_graph()\n",
        "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
        "    input = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
        "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
        "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
        "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
        "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
        "    return input, keep_prob, layer3, layer4, layer7\n",
        "\n",
        "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
        "    l7_conv = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_1',activation = tf.nn.relu)\n",
        "\n",
        "\n",
        "    conv1 = tf.layers.conv2d_transpose(l7_conv, num_classes, 4, 2,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_2',activation = tf.nn.relu)\n",
        "\n",
        "    l4_conv = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_3',activation = tf.nn.relu)\n",
        "\n",
        "    skip_1 = tf.add(conv1, l4_conv, name='conv_1_1_4')\n",
        "    conv2 = tf.layers.conv2d_transpose(skip_1, num_classes, 4, 2,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_5',activation = tf.nn.relu)\n",
        "    l3_conv = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, 1,\n",
        "                                       padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                       kernel_regularizer=tf.keras.regularizers.L2(0.01), name='conv_1_1_6',activation = tf.nn.relu)\n",
        "    skip_3 = tf.add(conv2, l3_conv,  name='conv_1_1_7')\n",
        "\n",
        "    output = tf.layers.conv2d_transpose(skip_3, num_classes, 16, 8,\n",
        "                                        padding='same', kernel_initializer= tf.random_normal_initializer(stddev=STDEV),\n",
        "                                        kernel_regularizer=tf.keras.regularizers.L2(0.01),  name='conv_1_1_8',activation = tf.nn.relu)\n",
        "\n",
        "    return output\n",
        "\n",
        "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
        "    \n",
        "\n",
        "    logits = tf.reshape(nn_last_layer, (-1, num_classes), name='logits')\n",
        "    labels = tf.reshape(correct_label, (-1, num_classes), name='labels')\n",
        "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(cross_entropy_loss)\n",
        "\n",
        "    return logits, train_op, cross_entropy_loss\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
        "                 correct_label, keep_prob, learning_rate, X_train, y_train, label_values, X_val, y_val):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print('epoch : ', epoch)\n",
        "        for image, targets in gbf(X_train, y_train, label_values, batch_size):\n",
        "            _, loss = sess.run([train_op, cross_entropy_loss],\n",
        "                               feed_dict={input_image: image, correct_label: targets, keep_prob: KEEP_PROB,\n",
        "                                          learning_rate: LEARNING_RATE})\n",
        "\n",
        "            print(\"Epoch: {}\".format(epoch + 1), \"/ {}\".format(epochs), \" Loss: {:.3f}\".format(loss))\n",
        "\n",
        "        mean_loss = []\n",
        "        for image, targets in get_batches_fn(X_val, y_val, label_values, batch_size, is_train=False):\n",
        "            loss = sess.run([cross_entropy_loss],\n",
        "                            feed_dict={input_image: image, correct_label: targets, keep_prob: 1})\n",
        "            mean_loss.append(loss)\n",
        "\n",
        "        mean_loss_ = np.mean(np.array(mean_loss))\n",
        "        print(\"Epoch: {}\".format(epoch + 1), \"/ {}\".format(epochs), \" Validation Loss: {:.3f}\".format(mean_loss_))\n",
        "        saver = tf.train.Saver()\n",
        "        print('saving model')\n",
        "        saver.save(sess, './model')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run():\n",
        "    num_classes = NUM_CLASSES\n",
        "    image_shape = (512, 256)  \n",
        "    data_dir = DATA_DIR\n",
        "    video_dir = VIDEO_DIR\n",
        "    runs_dir = RUNS_DIR\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    is_train = IS_TRAIN\n",
        "   \n",
        "    maybe_download_pretrained_vgg(data_dir)\n",
        "    label_values = get_label_info()\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        \n",
        "        vgg_path = os.path.join(data_dir, 'vgg')\n",
        "        \n",
        "        gbf = gen_batch_function\n",
        "\n",
        "        input, keep_prob, layer3, layer4, layer7 = load_vgg(sess, vgg_path)\n",
        "        print(input)\n",
        "        output = layers(layer3, layer4, layer7, num_classes)\n",
        "        correct_label = tf.placeholder(dtype=tf.float32, shape=(None, None, None, num_classes))\n",
        "        learning_rate = tf.placeholder(dtype=tf.float32)\n",
        "        logits, train_op, cross_entropy_loss = optimize(output, correct_label, learning_rate, num_classes)\n",
        "        logits = tf.nn.softmax(logits, name='softmax')\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver() \n",
        "\n",
        "        if not is_train:\n",
        "            saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
        "            save_inference_samples(runs_dir, video_dir, sess, image_shape, logits, keep_prob, input, label_values)\n",
        "        else:\n",
        "            X_train, y_train, X_val, y_val = get_data(data_dir)\n",
        "            train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input, correct_label,\n",
        "                         keep_prob, learning_rate, X_train, y_train, label_values, X_val, y_val)\n",
        "            save_inference_samples(runs_dir, video_dir, sess, image_shape, logits, keep_prob, input,\n",
        "                                                     label_values)\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIveBTfBWxXQ",
        "outputId": "2a8a3baf-683e-4e1f-97d4-aa37d1812a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.8.0\n",
            "Default GPU Device: /device:GPU:0\n",
            "run\n",
            "INFO:tensorflow:Restoring parameters from /content/train/vgg/variables/variables\n",
            "Tensor(\"image_input:0\", shape=(?, ?, ?, 3), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:575: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/convolutional.py:1736: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading X_Train..\n",
            "Loading Y_Train..\n",
            "Loading X_Validation..\n",
            "Loading Y_Validation..\n",
            "epoch :  0\n",
            "Epoch: 1 / 60  Validation Loss: nan\n",
            "saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  1\n",
            "Epoch: 2 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  2\n",
            "Epoch: 3 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  3\n",
            "Epoch: 4 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  4\n",
            "Epoch: 5 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  5\n",
            "Epoch: 6 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  6\n",
            "Epoch: 7 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  7\n",
            "Epoch: 8 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  8\n",
            "Epoch: 9 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  9\n",
            "Epoch: 10 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  10\n",
            "Epoch: 11 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  11\n",
            "Epoch: 12 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  12\n",
            "Epoch: 13 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  13\n",
            "Epoch: 14 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  14\n",
            "Epoch: 15 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  15\n",
            "Epoch: 16 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  16\n",
            "Epoch: 17 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  17\n",
            "Epoch: 18 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  18\n",
            "Epoch: 19 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  19\n",
            "Epoch: 20 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  20\n",
            "Epoch: 21 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  21\n",
            "Epoch: 22 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  22\n",
            "Epoch: 23 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  23\n",
            "Epoch: 24 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  24\n",
            "Epoch: 25 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  25\n",
            "Epoch: 26 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  26\n",
            "Epoch: 27 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  27\n",
            "Epoch: 28 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  28\n",
            "Epoch: 29 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  29\n",
            "Epoch: 30 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  30\n",
            "Epoch: 31 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  31\n",
            "Epoch: 32 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  32\n",
            "Epoch: 33 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  33\n",
            "Epoch: 34 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  34\n",
            "Epoch: 35 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  35\n",
            "Epoch: 36 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  36\n",
            "Epoch: 37 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  37\n",
            "Epoch: 38 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  38\n",
            "Epoch: 39 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  39\n",
            "Epoch: 40 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  40\n",
            "Epoch: 41 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  41\n",
            "Epoch: 42 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  42\n",
            "Epoch: 43 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  43\n",
            "Epoch: 44 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  44\n",
            "Epoch: 45 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  45\n",
            "Epoch: 46 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  46\n",
            "Epoch: 47 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  47\n",
            "Epoch: 48 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  48\n",
            "Epoch: 49 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  49\n",
            "Epoch: 50 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  50\n",
            "Epoch: 51 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  51\n",
            "Epoch: 52 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  52\n",
            "Epoch: 53 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  53\n",
            "Epoch: 54 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  54\n",
            "Epoch: 55 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  55\n",
            "Epoch: 56 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  56\n",
            "Epoch: 57 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  57\n",
            "Epoch: 58 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  58\n",
            "Epoch: 59 / 60  Validation Loss: nan\n",
            "saving model\n",
            "epoch :  59\n",
            "Epoch: 60 / 60  Validation Loss: nan\n",
            "saving model\n",
            "Saving test images to: runs/1650911934.1061327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('/content/saved_op')"
      ],
      "metadata": {
        "id": "RMO-R8hJtd2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}